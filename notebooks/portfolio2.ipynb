{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "usual-summit",
   "metadata": {
    "id": "designed-insulin"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-healing",
   "metadata": {
    "id": "GWMGsEDSzosM"
   },
   "source": [
    "# Portfolio 2 - The Rescorla-Wagner Model\n",
    "\n",
    "In this portfolio, we are going to implement the Rescorla-Wagner model to simulate the behavior of agents interacting with environments that can have different levels of volatility. The Rescorla-Wagner model is a model of classical conditioning in which learning occurs in response to a prediction error. The model itself is a simple algebraic equation describing how associative values between a stimulus and an outcome are updated via error correction. \n",
    "\n",
    "This exercise will also introduce you to concepts like *learning rate*, *prediction error*, *volatility*, and *uncertainty* that are central in the reinforcement learning literature and computational psychiatry.\n",
    "\n",
    "It is recommended that you first explore some resources to get a good intuition of the model before moving to the implementation part. The following videos are good introduction to the core concepts: [video-1](https://www.youtube.com/watch?v=D8b-cflPpec), [video-2](https://www.youtube.com/watch?v=CXrMtA1eNvQ).\n",
    "\n",
    "**Additional resources**\n",
    "\n",
    "> [Wikipedia](https://en.wikipedia.org/wiki/Rescorla%E2%80%93Wagner_model)\n",
    "\n",
    "> [Scholarpedia](http://www.scholarpedia.org/article/Rescorla-Wagner_model)\n",
    "\n",
    "> Sutton, R. & Barto, A. (2018). Reinforcement learning: an introduction. Cambridge, Massachusetts London, England: The MIT Press. *14.2.2. The Rescorla-Wagner Model*.\n",
    "\n",
    "* [BayesCog Summer 2020 Lecture 09 - Intro to cognitive modeling & Rescorla-Wagner model](https://www.youtube.com/watch?v=tXFKYWx6c3k)\n",
    "* [BayesCog Summer 2020 Lecture 10 - Implementing Rescorla-Wagner in Stan](https://www.youtube.com/watch?v=M69theIxI3g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-speech",
   "metadata": {
    "id": "_iVkYCjHBptK"
   },
   "source": [
    "## Learning rate\n",
    "\n",
    "In this exercise, we will create a simple version of the Rescorla-Wagner model that can update the associative value of a stimulus based on the prediction error of the outcome. We would like to reproduce the following figure from [Pulcu & Browning (2019)](https://doi.org/10.1016/j.tics.2019.07.007): \n",
    "\n",
    "![title](https://marlin-prod.literatumonline.com/cms/attachment/cdba6dca-87e1-480b-b7ee-d6f554ba8a72/gr1.jpg)\n",
    "\n",
    "The core idea here is that task volatility (the rapid change of probability outcomes associated with conditioned stimuli) requires an adaptation of the learning rate of a basic (Rescorla-Wagner) reinforcement learning model. You are going to demonstrate that by simulating the performances of 2 agents (high and low learning rate) performing 2 tasks (high and low cat volatility).\n",
    "\n",
    "Our agent cannot tell beforehand if the cat will scratch him or not, but he can learn from previous experience and update his expectation that he will be scratched in the future. Two key parameters are guiding the value update: the *prediction error* and the *learning rate*.\n",
    "\n",
    "The *prediction error* is parameterized as:\n",
    "\n",
    "$$ PE_{t-1} = R_{t-1} - V_{t-1} $$\n",
    "\n",
    "Updating the value function is defined by:\n",
    "\n",
    "$$ V_{t} = V_{t-1} + \\alpha * PE $$\n",
    "\n",
    "where $R$ is the outcome, $V$ is the associative strength between the CS and the outcome, and $\\alpha$ is the learning rate.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "You should create a Python class named `RescorlaWagner`, this class should have 3 core methods: \n",
    "* `response()` will generate the agent decision (e.g. stroke the cat or not).\n",
    "* `update()` will update the outcome (did the cat scratch the agent or not, update the prediction for the next trial).\n",
    "* `plot()` will plot the result from the entire experiment (hidden probability, responses, and associative value).\n",
    "\n",
    "The final plot might look like something like this (this is just for illustration, the final traces can differ):\n",
    "![title](https://github.com/LegrandNico/CognitiveModeling/raw/master/notebooks/data/learningRate.png)\n",
    "\n",
    "All data and relevant parameters should be stored in the class attributes. All methods should be documented following the Matplotlib standard (see [here](https://matplotlib.org/stable/devel/documenting_mpl.html)). You cannot import any additional package (just use Numpy and Matplotlib).\n",
    "\n",
    "The two cat volatility levels are given by the following vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-brake",
   "metadata": {
    "id": "__hOsoiduyDm"
   },
   "outputs": [],
   "source": [
    "volatileCat = np.repeat(\n",
    "    np.array([0.5, 0.8, 0.1, 0.8, 0.1, 0.8, 0.1, 0.8, 0.1, 0.8]),\n",
    "    10, axis=0)\n",
    "\n",
    "stableCat = np.repeat(\n",
    "    np.array([0.5, 0.8, 0.1, 0.8]),\n",
    "    25, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-creator",
   "metadata": {
    "id": "7iC9NRTZuzte"
   },
   "source": [
    "1) You should use two agents and make them learn the underlying associations using a high and a low learning rate (e.g 0.2 vs 0.8). You should have four plots in total (learning rate * task variability).\n",
    "\n",
    "2) How can we find an optimal learning rate that would maximize the outcome (stroke the cat without being scratched most of the time)? Run 1000 simulations and use a robust estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-joshua",
   "metadata": {
    "id": "9TwJ71k1Bp2v"
   },
   "source": [
    "## Blocking\n",
    "\n",
    "We have built from the previous exercise an agent that can update its expectation about future outcomes associated with a stimulus. However, one of the main strengths of the Rescorla-Wagner model, and the reason why it is still used as a baseline reference for a reinforcement learning task, is that it can explain results that occur under more complex setups like blocking.\n",
    "\n",
    "Blocking happens when many CSs are presented in association with an outcome, but only some of them show a conditioning response. In the previous example, we used only one CS (the cat) that was always presented, and the outcome probability was varying through time. Here, we are going to present 2 stimuli ($A$ and $X$).\n",
    "\n",
    "$V_{A}$, $V_{X}$ and $V_{AX}$ denote the associative strength of stimuli $A$, $X$ and the compound $AX$. Suppose that on a trial the compound $CS_{AX}$ is followed by a $US$ laballed stimulus $Y$. Then the associative strengths of the stimulus components change according to these expression:\n",
    "\n",
    "$$ \\Delta V_{A} = \\alpha_{A}\\beta_{Y}(R_{Y}-V_{AX})$$\n",
    "$$ \\Delta V_{X} = \\alpha_{X}\\beta_{Y}(R_{Y}-V_{AX})$$\n",
    "\n",
    "Here, $V_{AX} = V_{A} + V_{X}$. $\\alpha$ refers to the $CS$ salience and $\\beta$ refers to the $US$ salience, which can be 0 or 1 given the array below. $\\alpha_{a}$ is encoded in `stimulusA`, $\\alpha_{x}$ is encoded in `stimulusX` and $\\beta_{Y}$ is encoded on `unconditioned`. $R_{Y}$ is the asymptotic level of associative strength that the US $Y$ can support. In this case, this will be set to 1. Note that we are not explicitely describing the learning rate here, but this can be added by scaling $\\Delta V_{A}$ and $\\Delta V_{X}$ by some number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "rental-bennett",
   "metadata": {
    "id": "WRhO_EUrF0hI"
   },
   "outputs": [],
   "source": [
    "unconditioned = np.repeat(\n",
    "    np.array([0, 1, 0, 1, 0, 0]),\n",
    "    10, axis=0)\n",
    "\n",
    "stimulusA = np.repeat(\n",
    "    np.array([0, 1, 0, 1, 0, 0]),\n",
    "    10, axis=0)\n",
    "\n",
    "stimulusX = np.repeat(\n",
    "    np.array([0, 0, 0, 1, 0, 1]),\n",
    "    10, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-jacob",
   "metadata": {
    "id": "Y5SmjJpjVvdX"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "* Using the previous `RescorlaWagner` class, make some modifications that will let it use a compound of CS variables (here $A$ and $X$) to predict the unconditioned stimulus. Because the association $A-Y$ is learned first, we should observe a *blocking* of $X-Y$ when it is later presented.\n",
    "\n",
    "* Create a figure synthesizing the main result from this task, demonstrating that the Rescorla-Wagner model can explain the blocking effect. Ideally, the figure should have 5 rows:\n",
    "* Unconditioned stimulus $Y$\n",
    "* Stimulus $A$\n",
    "* Stimulus $X$\n",
    "* $V_{A}$\n",
    "* $V_{X}$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "portfolio2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

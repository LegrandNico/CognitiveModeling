{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "portfolio3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bAwraQ1_nqq"
      },
      "source": [
        "%%capture\n",
        "! pip install -U gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs1WQ_DI_whS"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "sns.set_context(\"talk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPmxHuuw_1vZ"
      },
      "source": [
        "# Portfolio 3 - Reinforcement learning\n",
        "\n",
        "This portfolio has two parts. The first part is theoretical/conceptual, you will be asked to define core concepts of reinforcement learning. Try to keep the responses clear and concise (no more than 4-5 sentences each). In the second part, you will have to implement reinforcement learning algorithms that can solve an OpenAI Gym environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt6dLIaUCS6b"
      },
      "source": [
        "## Theory\n",
        "\n",
        "1. **What is the credit assignment problem?**\n",
        ">R:\n",
        "\n",
        "2. **What is the exploration/exploitation tradeoff?**\n",
        ">R:\n",
        "\n",
        "3. **What is the Markov property?**\n",
        ">R:\n",
        "\n",
        "4. **What is the difference between an *episodic task* and a *continuing task*?**\n",
        ">R:\n",
        "\n",
        "5. **Write the formula describing the return that an agent can get in a *continuing task*.**\n",
        ">R:\n",
        "\n",
        "6. **What are the consequences if the discounting factor is set to $0$? How would you describe the behavior of a real agent (e.g. a human or an animal) using a discounting factor of $0$.**\n",
        ">R:\n",
        "\n",
        "7. **Same question, this time with a discounting factor set to $1$.**\n",
        ">R:\n",
        "\n",
        "8. **What is the difference between the value function and the action-value function?**\n",
        ">R:\n",
        "\n",
        "9. **What is an $\\epsilon -greedy$ policy?**\n",
        ">R:\n",
        "\n",
        "10. **Give at least two real-world examples of off-policy learning.**\n",
        ">R:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vny7zXOPsYO"
      },
      "source": [
        "## Practice\n",
        "\n",
        "In this exercise, you will have to provide various solutions for the MountainCar environment. You should create 5 different agents and compare their performances:\n",
        "\n",
        "* An agent that will sample action randomly at each time steps.\n",
        "* An agent that will follow hard-coded rules that could make it solve the problem (you have to create the rules).\n",
        "* An agent that will use a Q-learning algorithm.\n",
        "* An agent that will use a Deep Q Network (DQN).\n",
        "* An agent that will use a Double DQN.\n",
        "\n",
        "You might refer to:\n",
        "> GÃ©ron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O'Reilly Media, Inc. Ch.18. Reinforcement learning.\n",
        "\n",
        "for details on how to implement such algorithms. Some parameters like the batch size, the shape of the neural network, the way rewards are provided, the discretization of the observations, etc... can largely affect the performance our your agent, so this is something you might try to fine-tune. Note, however, that the final performance of the agent is not what is being evaluated here, but the code and the model itself.\n",
        "\n",
        "Make sure that your code is fully documented, and that the variable names are consistent.\n",
        "\n",
        "The final result will be a plot of the performance of your 5 agents through 1000 episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6UMx3mSPt-h"
      },
      "source": [
        "env = gym.make(\"MountainCar-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMXDZxP_gg7a"
      },
      "source": [
        "## Random agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN1VDoXEFn0m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOxBwQ7ChXUR"
      },
      "source": [
        "## Hard-coded solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyLr2ydxgl2o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_P1Q7WThwZQ"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfEFW37cFo8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fbE7mSahzqA"
      },
      "source": [
        "## Deep Q Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0IbqsZTFrR2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39tcKoENIBm7"
      },
      "source": [
        "## Double Deep Q Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sirJxdb2IB9C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7tS5o4Opi19"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWtOac1krBP3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}